{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d099ec7a-5cf2-4a68-b277-6c7fbb39c2d8",
   "metadata": {},
   "source": [
    "### Q1: What are the Probability Mass Function (PMF) and Probability Density Function (PDF)? Explain with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8ab7c2-6c8f-4dc5-988f-b94135aec21b",
   "metadata": {},
   "source": [
    "#### 1.Probability Mass Function (PMF):\n",
    "The PMF is a function that maps each possible value of a discrete random variable to its probability of occurrence. It represents the probability that a random variable takes a certain value. The sum of probabilities of all possible values of the random variable is equal to 1. The PMF is defined as:\n",
    "\n",
    "PMF(x) = P(X = x)\n",
    "\n",
    "where X is a random variable, x is a possible value that X can take, and P(X = x) is the probability that X takes the value x.\n",
    "\n",
    "#### Example: \n",
    "Consider the roll of a fair six-sided die. The possible values that the die can take are 1, 2, 3, 4, 5, and 6, each with a probability of 1/6. The PMF of the random variable X, which represents the number on the top face of the die, is:\n",
    "\n",
    "PMF(1) = 1/6, PMF(2) = 1/6, PMF(3) = 1/6, PMF(4) = 1/6, PMF(5) = 1/6, PMF(6) = 1/6\n",
    "\n",
    "\n",
    "#### Probability Density Function (PDF):\n",
    "PDF is a function that describes the probability density of a continuous random variable. It represents the probability that a random variable falls within a certain range of values. The area under the curve of the PDF between any two points represents the probability that the random variable falls within that range. The integral of the PDF over the entire domain is equal to 1. The PDF is defined as:\n",
    "\n",
    "PDF(x) = dF(x)/dx\n",
    "\n",
    "#### Example: \n",
    "Consider a normal distribution with a mean of 0 and a standard deviation of 1. The PDF of the random variable X, which represents the values that X can take, is:\n",
    "\n",
    "PDF(x) = 1/(sqrt(2pi)) exp(-(x^2)/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22b7c69-c114-4047-97b9-f32679c3710f",
   "metadata": {},
   "source": [
    "### Q2: What is Cumulative Density Function (CDF)? Explain with an example. Why CDF is used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7ef692-1708-4642-aa8b-689e5d4169f3",
   "metadata": {},
   "source": [
    "A Cumulative Density Function (CDF) is a mathematical function that describes the cumulative probability distribution of a random variable. In other words, it shows the probability that a random variable takes on a value less than or equal to a given value. CDFs are commonly used in probability theory and statistics to understand and analyze the behavior of random variables.\n",
    "\n",
    "The CDF of a random variable X, denoted as F(x), is defined as:\n",
    "\n",
    "F(x) = P(X ≤ x)\n",
    "\n",
    "Here's an explanation of each component:\n",
    "- F(x): The CDF at a specific value x.\n",
    "- X: The random variable.\n",
    "- P(X ≤ x): The probability that the random variable X takes on a value less than or equal to x.\n",
    "\n",
    "To illustrate this concept with an example, let's consider a simple scenario involving the roll of a fair six-sided die. In this case, the random variable X represents the outcome of the die roll.\n",
    "\n",
    "The CDF for this random variable X would look like this:\n",
    "\n",
    "F(x) = 0 for x < 1\n",
    "F(x) = 1/6 for 1 ≤ x < 2\n",
    "F(x) = 2/6 for 2 ≤ x < 3\n",
    "F(x) = 3/6 for 3 ≤ x < 4\n",
    "F(x) = 4/6 for 4 ≤ x < 5\n",
    "F(x) = 5/6 for 5 ≤ x < 6\n",
    "F(x) = 1 for x ≥ 6\n",
    "\n",
    "Now, let's explain why CDFs are used:\n",
    "\n",
    "1. Understanding Probability Distribution\n",
    "2. Calculating Probabilities\n",
    "3. Comparing Random Variables\n",
    "4. Statistical Analysis\n",
    "5. Modeling and Simulation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809ced7e-98bb-4221-932d-2c1f3756fd80",
   "metadata": {},
   "source": [
    "#### Q3: What are some examples of situations where the normal distribution might be used as a model? Explain how the parameters of the normal distribution relate to the shape of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1e82a1-e1d8-4abe-8149-05788f29e2ef",
   "metadata": {},
   "source": [
    "The normal distribution, also known as the Gaussian distribution or bell curve, is a widely used probability distribution in various fields due to its simplicity and applicability to a wide range of situations. It is characterized by two parameters: the mean (μ) and the standard deviation (σ). The shape of the normal distribution is determined by these parameters, and it has several properties that make it suitable for modeling various real-world phenomena. Here are some examples of situations where the normal distribution might be used as a model:\n",
    "\n",
    "1. **Height of Individuals:** \n",
    "2. **Test Scores:**\n",
    "3. **Measurement Errors:**\n",
    "4. **Financial Returns:** \n",
    "\n",
    "Parameters of the Normal Distribution and Their Relation to Shape:\n",
    "\n",
    "1. **Mean (μ):** The mean represents the central location or the average of the distribution. It is the point around which the curve is symmetrically centered. Shifting the mean to the left or right will change the central location of the distribution without altering its shape.\n",
    "\n",
    "2. **Standard Deviation (σ):** The standard deviation measures the spread or dispersion of the data. A smaller standard deviation results in a narrower and taller curve, indicating less variability. A larger standard deviation leads to a wider and flatter curve, suggesting more variability in the data.\n",
    "\n",
    "The normal distribution is completely characterized by its mean and standard deviation. Changing these parameters can create variations in the shape of the curve, but it will always maintain its characteristic bell-shaped, symmetrical form. The empirical rule, also known as the 68-95-99.7 rule, illustrates how changes in the standard deviation affect the percentage of data within certain intervals from the mean:\n",
    "\n",
    "- Approximately 68% of the data falls within one standard deviation of the mean.\n",
    "- Approximately 95% falls within two standard deviations.\n",
    "- Approximately 99.7% falls within three standard deviations.\n",
    "\n",
    "This makes the normal distribution a valuable tool for understanding and analyzing data in a wide range of applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aec116b-a650-4e57-9c5c-433c357cbc06",
   "metadata": {},
   "source": [
    "### Q4: Explain the importance of Normal Distribution. Give a few real-life examples of Normal Distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3afa1c-74c2-4aa0-aa16-383fb66cd816",
   "metadata": {},
   "source": [
    "The importance of the normal distribution can be explained as follows:\n",
    "\n",
    "**1.Many natural phenomena follow a normal distribution:** The normal distribution is observed in many natural phenomena, such as the heights and weights of a population, the measurement errors in laboratory experiments, the test scores of students, the time taken to complete a task, and many more.\n",
    "\n",
    "**2.Statistical inference:** The normal distribution is used extensively in statistical inference, which involves drawing conclusions about a population based on a sample of data. Many statistical tests, such as the t-test and ANOVA, assume that the data follow a normal distribution.\n",
    "\n",
    "**3.Central limit theorem:** The central limit theorem states that the sum of a large number of independent and identically distributed random variables will tend to a normal distribution, regardless of the distribution of the individual variables. This property makes the normal distribution a fundamental concept in probability theory and statistics.\n",
    "Some real-life examples of the normal distribution are:\n",
    "\n",
    "**1.Heights of adults:** The heights of adults in a population follow a normal distribution, with a mean of around 5 feet 7 inches and a standard deviation of around 3 inches.\n",
    "\n",
    "**2.Test scores:** The scores on a standardized test, such as the SAT or GRE, follow a normal distribution, with a mean of around 500-600 and a standard deviation of around 100-200.\n",
    "\n",
    "**3.Body temperature:** The body temperature of a healthy human follows a normal distribution, with a mean of around 98.6 degrees Fahrenheit and a standard deviation of around 0.5 degrees.\n",
    "\n",
    "**3.IQ scores:** The IQ scores of a population follow a normal distribution, with a mean of 100 and a standard deviation of 15.\n",
    "\n",
    "**4.Stock market returns:** The daily returns on the stock market follow a normal distribution, with a mean of around 0 and a standard deviation of around 1-2%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead51ab4-c899-4db1-9919-9ba2676998a9",
   "metadata": {},
   "source": [
    "### Q5: What is Bernaulli Distribution? Give an Example. What is the difference between Bernoulli Distribution and Binomial Distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9984fa7d-d287-4587-9e6f-4bdd3810531c",
   "metadata": {},
   "source": [
    "The Bernoulli distribution is a discrete probability distribution that models a random experiment with exactly two possible outcomes: success and failure. It is named after the Swiss mathematician Jacob Bernoulli. The distribution is characterized by a single parameter, often denoted as \"p,\" which represents the probability of success. The probability of failure is complementary to p, meaning it's equal to 1 - p.\n",
    "\n",
    "Mathematically, the probability mass function (PMF) of the Bernoulli distribution is as follows:\n",
    "\n",
    "P(X = 1) = p (for success)\n",
    "P(X = 0) = 1 - p (for failure)\n",
    "\n",
    "Here's an example of a Bernoulli distribution:\n",
    "\n",
    "**Example:** Consider a simple experiment of flipping a fair coin, where success (1) represents getting heads, and failure (0) represents getting tails. The probability of getting heads (success) on a fair coin is p = 0.5, and the probability of getting tails (failure) is 1 - p = 0.5. In this case, the experiment follows a Bernoulli distribution with p = 0.5.\n",
    "\n",
    "Now, let's discuss the key differences between the Bernoulli distribution and the Binomial distribution:\n",
    "\n",
    "1. **Number of Trials:**\n",
    "   - **Bernoulli Distribution:** It models a single trial or experiment with two possible outcomes (success and failure).\n",
    "   - **Binomial Distribution:** It models the number of successes in a fixed number of independent Bernoulli trials (experiments). In other words, it deals with multiple trials, each of which follows a Bernoulli distribution.\n",
    "\n",
    "2. **Parameters:**\n",
    "   - **Bernoulli Distribution:** It has only one parameter, p, which represents the probability of success in a single trial.\n",
    "   - **Binomial Distribution:** It has two parameters: n (the number of trials) and p (the probability of success in each trial).\n",
    "\n",
    "3. **Random Variable:**\n",
    "   - **Bernoulli Distribution:** The random variable, X, in a Bernoulli distribution takes on values of 0 (failure) or 1 (success) for a single trial.\n",
    "   - **Binomial Distribution:** The random variable, X, in a Binomial distribution represents the number of successes in n independent Bernoulli trials. It can take on values from 0 to n.\n",
    "\n",
    "4. **Probability Mass Function (PMF):**\n",
    "   - **Bernoulli Distribution:** The PMF of the Bernoulli distribution describes the probabilities of success and failure for a single trial.\n",
    "   - **Binomial Distribution:** The PMF of the Binomial distribution calculates the probabilities of getting k successes in n trials, where k can vary from 0 to n.\n",
    "\n",
    "5. **Use Cases:**\n",
    "   - **Bernoulli Distribution:** It is used to model simple, binary experiments like coin flips, where there are only two possible outcomes.\n",
    "   - **Binomial Distribution:** It is used to model situations where you want to find the probability of a specific number of successes in a fixed number of repeated Bernoulli trials. Examples include the probability of getting a certain number of heads in multiple coin flips or the probability of passing a certain number of tests out of a series of identical tests.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5109a4-6d44-400d-a3a0-be64682d990a",
   "metadata": {},
   "source": [
    "### Q6. Consider a dataset with a mean of 50 and a standard deviation of 10. If we assume that the dataset is normally distributed, what is the probability that a randomly selected observation will be greater than 60? Use the appropriate formula and show your calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ace327-cb3d-4357-8c54-3127012619d1",
   "metadata": {},
   "source": [
    "We can use the z-table to solve this question.\n",
    "\n",
    "The appropriate formula to use is:\n",
    "\n",
    "**z = (x - μ) / σ**\n",
    "\n",
    "where x is the value of the observation you are interested in (in this case, x = 60), μ is the mean of the dataset, and σ is the standard deviation of the dataset.\n",
    "\n",
    "Substituting the values given in the question, we get:\n",
    "z = (60 - 50) / 10 = 1\n",
    "\n",
    "Now, we need to use the z-table to find the probability that a z-score is greater than 1.\n",
    "\n",
    "Using the z-table, we look up the probability corresponding to a z-score of 1.00 in the positive z-score column. The table tells us that the probability is 0.8413.\n",
    "\n",
    "The probabilty that we got from z-table is the probability of randomly selected number less than 60, because z-table gives us the probability of the values on the left side of 60. But we need the values on the right side (greater than 60) of 60. So we can get that probability by subtracting 0.8413 from 1 as:\n",
    "\n",
    "1 - 0.8413 = 0.1587"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e28d5e-f9e2-4496-878a-14843c484d2a",
   "metadata": {},
   "source": [
    "### Q7: Explain uniform Distribution with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f7dede-9cc3-43ac-abbb-e15eac4c02ec",
   "metadata": {},
   "source": [
    "Uniform distribution, also known as a rectangular distribution, is a probability distribution where all possible outcomes are equally likely to occur. It is often used in statistics to model situations where each outcome is equally likely to occur, such as rolling a fair die or picking a card from a well-shuffled deck.\n",
    "\n",
    "Example:\n",
    "Rolling a fair six-sided die: When rolling the die, each face has an equal probability of showing up, which is 1/6 or approximately 0.1667. This means that any number between 1 and 6 is equally likely to be rolled, and the probability of rolling any particular number is 1/6.\n",
    "\n",
    "In the above examples, the probability density function of the uniform distribution is constant over the entire range of possible outcomes. That is, the probability of any particular outcome is proportional to the size of the range of possible outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6239a971-8b23-4a2f-8050-65e3dba25bdd",
   "metadata": {},
   "source": [
    "### Q8: What is the z score? State the importance of the z score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc3b9a7-9099-41b0-b5ab-7321ef8f73ae",
   "metadata": {},
   "source": [
    "The z-score is a statistical measure that expresses how far a data point is from the mean of a distribution in terms of standard deviations.\n",
    "\n",
    "The formula for calculating the z-score of a data point is:\n",
    "\n",
    "z = (x - μ) / σ\n",
    "\n",
    "where x is the data point, μ is the mean of the distribution, and σ is the standard deviation.\n",
    "\n",
    "Importance:\n",
    "\n",
    "The z-score is important because it allows us to standardize data from different distributions, which can then be compared and analyzed more easily. By converting data into z-scores, we can compare observations from different samples or populations and make meaningful statements about their relative positions.\n",
    "The z-score is also useful in hypothesis testing, where it is used to calculate the probability of observing a value as extreme as the one observed, assuming a certain null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f82c475-8586-4968-a62b-1485dfa9a698",
   "metadata": {},
   "source": [
    "### Q9: What is Central Limit Theorem? State the significance of the Central Limit Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fa7b02-69a0-4e32-8ccd-ac2db4ef6515",
   "metadata": {},
   "source": [
    "The Central Limit Theorem (CLT) is a fundamental concept in statistics that describes the distribution of sample means from a population, regardless of the shape of the population's distribution. It is a crucial theorem with significant implications for statistical analysis.\n",
    "\n",
    "Here's a formal statement of the Central Limit Theorem:\n",
    "\n",
    "\n",
    "**Normal Distribution Approximation:** The Central Limit Theorem allows us to approximate the distribution of sample means with a normal distribution. This is incredibly useful because many statistical methods and hypothesis tests are based on the assumption of a normal distribution.\n",
    "\n",
    "**Population Inference:** It enables us to make inferences about population parameters (such as the population mean) based on the properties of sample means. This is the foundation of inferential statistics.\n",
    "\n",
    "**Sample Size Flexibility:** The Central Limit Theorem doesn't require that the population itself follows a normal distribution. It only requires a sufficiently large sample size to hold. This flexibility is valuable in practical applications because real-world data often deviates from perfect normality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae662c9-764d-4b5b-9e37-fd97ac5c21c4",
   "metadata": {},
   "source": [
    "### Q10: State the assumptions of the Central Limit Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f554f1a0-70e4-44e5-9d10-aa849efd4e29",
   "metadata": {},
   "source": [
    "The assumptions of the Central Limit Theorem are:\n",
    "\n",
    "**Independence:** The observations in the sample are independent of each other, meaning that the outcome of one observation does not influence the outcome of another observation.\n",
    "\n",
    "**Sample size:** The sample size is sufficiently large. The larger the sample size, the better the approximation to the normal distribution.\n",
    "\n",
    "**Identically distributed:** The sample data comes from a population that has a well-defined mean and variance. The observations in the sample are identically distributed, meaning that they come from the same population.\n",
    "\n",
    "**Finite variance:** The population has a finite variance. This assumption ensures that the sample variance is also finite.\n",
    "\n",
    "**Non-skewed population distribution:** The population distribution is not strongly skewed. A strongly skewed population distribution can affect the validity of the Central Limit Theorem, and a larger sample size may be required to approximate a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b5f5b8-c655-4e8c-b391-b3b3c6d4398c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
